{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hellow'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hellow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response generated:  outside of this shallow [MASK]?\n",
      "\n",
      "I'm not sure\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertTokenizer, BertForMaskedLM\n",
    "\n",
    "def gpt2_generate_response(input_text):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=15)\n",
    "    generated_text = tokenizer.decode(output[-1], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def bert_generate_text(input_text):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=15, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "user_input = 'outside of this shallow [MASK]?' # [MASK] --> dark\n",
    "response = gpt2_generate_response(user_input)\n",
    "print('response generated: ',response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,  1101,   257,  1310,  1643,   286,   257, 34712,    13,   314,\n",
       "          1101,   257,  1263, 34712,    13,   314,  1101,   257,  1263, 34712,\n",
       "            13,   314,  1101,   257,  1263, 34712,    13,   314,  1101,   257,\n",
       "          1263, 34712,    13,   314,  1101,   257,  1263, 34712,    13,   314,\n",
       "          1101,   257]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "output = tokenizer.encode(\"I'm a little bit of a nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a\", return_tensors=\"pt\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm a little bit of a nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def predict_masked_word(sentence: str):\n",
    "    # Tokenize input sentence and find the index of the masked token\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\") # masked value to some kind tensor integer\n",
    "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "    \n",
    "    # Predict the masked token\n",
    "    output = model(input_ids)\n",
    "    \n",
    "    # Get the logits for the masked token and find the top prediction\n",
    "    mask_token_logits = output.logits[0, mask_token_index, :]\n",
    "    top_token_id = torch.argmax(mask_token_logits, dim=-1)\n",
    "    \n",
    "    # Decode the predicted token ID back to a word\n",
    "    predicted_token = tokenizer.decode(top_token_id)\n",
    "    return predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: water\n"
     ]
    }
   ],
   "source": [
    "sentence = \"outside this shallow [MASK]?\"\n",
    "predicted_word = predict_masked_word(sentence)\n",
    "print(f\"Predicted word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "def gpt2_generate_response(input_text):\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    output = model.generate(input_ids, max_length=65, num_return_sequences=1, temperature=0.99)\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Job\\RefonteInfiniti.com\\Foundation Models\\demos\\batch2DS\\backend\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.99` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a time when the world was still a little bit more open to the idea of a world where people could live in peace and harmony.\n",
      "\n",
      "\"But now, we have a world where people are not afraid to be themselves. We have a world where people are not afraid to be themselves. We have a world\n"
     ]
    }
   ],
   "source": [
    "input_text = \"There was a time\"\n",
    "generated_text = gpt2_generate_response(input_text)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Fine Tuning ----------------------\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        \n",
    "        for text in texts:\n",
    "            tokenized_text = tokenizer.encode(text, truncation=True, max_length=max_length)\n",
    "            self.input_ids.append(tokenized_text) # [[74, 837,343,54342,45443,323,], [742, 37,33,542,4443,323,]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.input_ids[idx])\n",
    "\n",
    "# Example input texts (split your data into smaller chunks if necessary)\n",
    "texts = [\n",
    "    \"Your very long input text here. \" * 100,  # Repeat to simulate a long text\n",
    "    \"Another example of a long input text.\" * 100\n",
    "]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(texts, tokenizer, 1024)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Fine-tuning parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss - 8.416922569274902\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch.squeeze(0)  # Remove batch dimension\n",
    "        outputs = model(input_ids[:-1], labels=input_ids[1:])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break\n",
    "    print(f\"Epoch {epoch+1}: Loss - {loss.item()}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine_tuned_gpt2\\\\tokenizer_config.json',\n",
       " 'fine_tuned_gpt2\\\\special_tokens_map.json',\n",
       " 'fine_tuned_gpt2\\\\vocab.json',\n",
       " 'fine_tuned_gpt2\\\\merges.txt',\n",
       " 'fine_tuned_gpt2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a time when the world was still a little bit more open to the idea of a world where people could live in peace and harmony.\n",
      "\n",
      "\"But now, we have a world where people are not afraid to be themselves. We have\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2\")\n",
    "finetuned_tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_gpt2\")\n",
    "\n",
    "input_text = \"There was a time\"\n",
    "input_ids = finetuned_tokenizer.encode(input_text, return_tensors='pt')\n",
    "output = finetuned_model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(finetuned_tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "# how you can dedply a gen_ai app on huggingface.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Job\\RefonteInfiniti.com\\Foundation Models\\demos\\batch2DS\\backend\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# # Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# device = torch.device(\"cuda\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "block_size = 8\n",
    "batch_size = 8\n",
    "\n",
    "with open('shakepeare_s_plays.txt', 'r') as f:\n",
    "  input_text = f.read()\n",
    "\n",
    "texts = input_text.split('\\n')\n",
    "dataset = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches():\n",
    "    indexes = torch.randint(len(dataset[0]) - block_size, (batch_size, )) # [1000111:1000111+block_size, 9:9+block_size, 1823789127, 13912]\n",
    "    x = torch.stack([dataset[0][i:i+block_size] for i in indexes])\n",
    "    y = torch.stack([dataset[0][i+1:i+block_size+1] for i in indexes]) # [1000112:1000112+block_size, 10:10+block_size, 1823789127, 13912]\n",
    "    # x = x.to(device)\n",
    "    # y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss - 7.863354206085205\n",
      "Epoch 2: Loss - 7.564795017242432\n",
      "Epoch 3: Loss - 7.337238311767578\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "# Fine-tuning loop \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # print(dataloader)\n",
    "    for batch in range(batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = batches() # Remove batch dimension\n",
    "        # print(input_ids)\n",
    "        outputs = model(x, labels=y)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: Loss - {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine_tuned_gpt2_\\\\tokenizer_config.json',\n",
       " 'fine_tuned_gpt2_\\\\special_tokens_map.json',\n",
       " 'fine_tuned_gpt2_\\\\vocab.json',\n",
       " 'fine_tuned_gpt2_\\\\merges.txt',\n",
       " 'fine_tuned_gpt2_\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_gpt2_\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2_\")\n",
    "finetuned_tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_gpt2_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a time when I was a little bit of a kid, and I was a little bit\n"
     ]
    }
   ],
   "source": [
    "input_text = \"There was a time\"\n",
    "input_ids = finetuned_tokenizer.encode(input_text, return_tensors='pt')\n",
    "output = finetuned_model.generate(input_ids, max_length=20, num_return_sequences=1)\n",
    "\n",
    "print(finetuned_tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
